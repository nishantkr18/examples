{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://lab.mlpack.org/v2/gh/mlpack/examples/master?urlpath=lab%2Ftree%2Fq_learning%2Fpendulum_dqn.ipynb)\n",
    "\n",
    "You can easily run this notebook at https://lab.mlpack.org/\n",
    "\n",
    "Here, we train a [Simple DQN](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) agent to get high scores for the [Pendulum](https://gym.openai.com/envs/Pendulum-v0/) environment. \n",
    "\n",
    "We make the agent train and test on OpenAI Gym toolkit's GUI interface provided through a distributed infrastructure (TCP API). More details can be found [here](https://github.com/zoq/gym_tcp_api).\n",
    "\n",
    "A video of the trained agent can be seen in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including necessary libraries and namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/core.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/methods/ann/ffn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_learning.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_networks/simple_dqn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/environment/env_type.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/training_config.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to run the agent on gym's environment (provided externally) for testing.\n",
    "#include <gym/environment.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to generate and display a video of the trained agent.\n",
    "#include \"xwidgets/ximage.hpp\"\n",
    "#include \"xwidgets/xvideo.hpp\"\n",
    "#include \"xwidgets/xaudio.hpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::ann;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace ens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::rl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the state and action space.\n",
    "DiscreteActionEnv::State::dimension = 3;\n",
    "DiscreteActionEnv::Action::size = 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the network.\n",
    "FFN<MeanSquaredError<>, GaussianInitialization> network(\n",
    "    MeanSquaredError<>(), GaussianInitialization(0, 1));\n",
    "network.Add<Linear<>>(DiscreteActionEnv::State::dimension, 128);\n",
    "network.Add<ReLULayer<>>();\n",
    "network.Add<Linear<>>(128, DiscreteActionEnv::Action::size);\n",
    "SimpleDQN<> model(network);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the policy and replay method.\n",
    "GreedyPolicy<DiscreteActionEnv> policy(1.0, 1000, 0.1, 0.99);\n",
    "RandomReplay<DiscreteActionEnv> replayMethod(32, 10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up training configurations.\n",
    "TrainingConfig config;\n",
    "config.ExplorationSteps() = 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up DQN agent.\n",
    "QLearning<DiscreteActionEnv, decltype(model), AdamUpdate, decltype(policy)>\n",
    "    agent(config, model, policy, replayMethod);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the gym training environment.\n",
    "gym::Environment env(\"gym.kurg.org\", \"4040\", \"Pendulum-v0\");\n",
    "\n",
    "// Initializing training variables.\n",
    "std::vector<double> returnList;\n",
    "size_t episodes = 0;\n",
    "bool converged = true;\n",
    "\n",
    "// The number of episode returns to keep track of.\n",
    "size_t consecutiveEpisodes = 50;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Pendulum environment has a continuous action space, we need to perform \"discretization\" of the action space.\n",
    "\n",
    "For that, we assume that our Q learning agent outputs 3 action values for our actions {0, 1, 2}. Meaning the actions given by the agent will either be `0`, `1`, or `2`. \n",
    "\n",
    "Now, we subtract `1.0` from the actions, which then becomes the input to the environment. This essentially means that we correspond the actions `0`, `1`, and `2` given by the agent, to the torque values `-1.0`, `0`, and `1.0` for the environment, respectively.\n",
    "\n",
    "This simple trick allows us to train a continuous action-space environment using DQN.\n",
    "\n",
    "Note that we have divided the action-space into 3 divisions here. But you may use any number of divisions as per your choice. More the number of divisions, finer are the controls available for the agent, and therefore better are the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Function to train the agent on the Pendulum gym environment.\n",
    "void train(const size_t numSteps)\n",
    "{\n",
    "  agent.Deterministic() = false;\n",
    "  std::cout << \"Training for \" << numSteps << \" steps.\" << std::endl;\n",
    "  while (agent.TotalSteps() < numSteps)\n",
    "  {\n",
    "    double episodeReturn = 0;\n",
    "    env.reset();\n",
    "    do\n",
    "    {\n",
    "      agent.State().Data() = env.observation;\n",
    "      agent.SelectAction();\n",
    "      arma::mat action = {double(agent.Action().action) - 1.0};\n",
    "\n",
    "      env.step(action);\n",
    "      DiscreteActionEnv::State nextState;\n",
    "      nextState.Data() = env.observation;\n",
    "\n",
    "      replayMethod.Store(agent.State(), agent.Action(), env.reward, nextState,\n",
    "          env.done, 0.99);\n",
    "      episodeReturn += env.reward;\n",
    "      agent.TotalSteps()++;\n",
    "      if (agent.Deterministic() || agent.TotalSteps() < config.ExplorationSteps())\n",
    "        continue;\n",
    "      agent.TrainAgent();\n",
    "    } while (!env.done);\n",
    "    returnList.push_back(episodeReturn);\n",
    "    episodes += 1;\n",
    "\n",
    "    if (returnList.size() > consecutiveEpisodes)\n",
    "      returnList.erase(returnList.begin());\n",
    "        \n",
    "    double averageReturn = std::accumulate(returnList.begin(),\n",
    "                                           returnList.end(), 0.0) /\n",
    "                           returnList.size();\n",
    "    if(episodes % 4 == 0)\n",
    "    {\n",
    "      std::cout << \"Avg return in last \" << returnList.size()\n",
    "          << \" episodes: \" << averageReturn\n",
    "          << \"\\t Episode return: \" << episodeReturn\n",
    "          << \"\\t Total steps: \" << agent.TotalSteps() << std::endl;\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the training begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps.\n",
      "Avg return in last 4 episodes: -1296.68\t Episode return: -1034.58\t Total steps: 800\n",
      "Avg return in last 8 episodes: -1201.89\t Episode return: -1023.34\t Total steps: 1600\n",
      "Avg return in last 12 episodes: -1137.23\t Episode return: -1229.57\t Total steps: 2400\n",
      "Avg return in last 16 episodes: -1089.89\t Episode return: -795.063\t Total steps: 3200\n",
      "Avg return in last 20 episodes: -1045.96\t Episode return: -740.987\t Total steps: 4000\n",
      "Avg return in last 24 episodes: -1018.07\t Episode return: -985.838\t Total steps: 4800\n"
     ]
    }
   ],
   "source": [
    "// Training the agent for a total of at least 5000 steps.\n",
    "train(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 200\t Total reward: -633.678\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42545db728804d63bc9296ef273506d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"Pendulum-v0\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action().action) - 1.0};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little more training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps.\n",
      "Avg return in last 28 episodes: -1014.36\t Episode return: -1070.43\t Total steps: 5600\n",
      "Avg return in last 32 episodes: -1013.11\t Episode return: -818.925\t Total steps: 6400\n",
      "Avg return in last 36 episodes: -991.138\t Episode return: -1008.86\t Total steps: 7200\n",
      "Avg return in last 40 episodes: -970.334\t Episode return: -745.247\t Total steps: 8000\n",
      "Avg return in last 44 episodes: -953.705\t Episode return: -757.426\t Total steps: 8800\n",
      "Avg return in last 48 episodes: -967.958\t Episode return: -1106.08\t Total steps: 9600\n",
      "Avg return in last 50 episodes: -945.132\t Episode return: -1155.99\t Total steps: 10400\n",
      "Avg return in last 50 episodes: -921.76\t Episode return: -1022.33\t Total steps: 11200\n",
      "Avg return in last 50 episodes: -911.658\t Episode return: -928.998\t Total steps: 12000\n",
      "Avg return in last 50 episodes: -904.935\t Episode return: -881.218\t Total steps: 12800\n",
      "Avg return in last 50 episodes: -902.272\t Episode return: -967.976\t Total steps: 13600\n",
      "Avg return in last 50 episodes: -909.005\t Episode return: -853.674\t Total steps: 14400\n",
      "Avg return in last 50 episodes: -906.569\t Episode return: -716.41\t Total steps: 15200\n",
      "Avg return in last 50 episodes: -880.956\t Episode return: -731.681\t Total steps: 16000\n",
      "Avg return in last 50 episodes: -863.457\t Episode return: -480.585\t Total steps: 16800\n",
      "Avg return in last 50 episodes: -853.463\t Episode return: -706.094\t Total steps: 17600\n",
      "Avg return in last 50 episodes: -847.844\t Episode return: -865.132\t Total steps: 18400\n",
      "Avg return in last 50 episodes: -819.985\t Episode return: -1164.12\t Total steps: 19200\n",
      "Avg return in last 50 episodes: -812.372\t Episode return: -685.779\t Total steps: 20000\n"
     ]
    }
   ],
   "source": [
    "// Training the same agent for a total of at least 20000 steps.\n",
    "train(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final agent testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 200\t Total reward: -119.403\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7e2de52b8646f3bccca8566a0b9fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"Pendulum-v0\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action().action) - 1.0};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
